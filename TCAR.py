import re
import string
import pyarabic.araby as araby
import functools
import operator
import re
import emoji


# Define the printable need to be removed from the text:
string.printable = string.printable + 'Ù¡Ù¢Ù£Ù¤Ù¥Ù¦Ù§Ù¨Ù©Ù ØŸØŒØ›' + 'â€¦' + 'ğŸ‡¦' + 'ğŸ‡¸' + 'ğŸ‡©' \
    + 'ğŸ‡«' + 'ğŸ‡°' + 'ğŸ‡·' + 'ğŸ‡¼' + 'ğŸ‡¦' + 'ğŸ‡­' + 'ğŸ‡²' + 'ğŸ‡¹' + 'ğŸ‡¾' + 'Ùª'

string.punctuation = string.punctuation + 'ØŒØŸØŒØ›'


table_printable = str.maketrans(string.printable, ' ' * len(string.printable))
table_punctuation = str.maketrans(string.punctuation, ' ' * len(string.punctuation))

# define the Harakat:
re_hamzated_alif = re.compile(r'[\u0622\u0623\u0625]')
re_alifMaqsura = re.compile(r'[\u0649]')
re_taaMarbota = re.compile(r'[\u0629]')
re_waw_hamzah = re.compile(r'[\u0624]')
re_alifMaqsurawithHamzah = re.compile(r'[\u0626]')



# norm will be used to clean similar letters that might be written in two different ways.
# Examples:
# Ø¢ Ø§
# ÙŠ Ù‰

def norm(token):
    
    """
        normalize the word by replacing hamzated Alif
        with Alif replacing AlifMaqsura with Yaa and replacing Taa Marbota with Haa
        """
    
    # replace Hamzated Alif with Alif bare
    token = re_hamzated_alif.sub('\u0627', token)
    # replace alifMaqsura with Yaa
    token = re_alifMaqsura.sub('\u064A', token)
    # replace Taa Marbota with Haa
    token = re_taaMarbota.sub('\u0647', token)
    # replcae waw hamzah with normal waw
    token = re_waw_hamzah.sub('\u0648' , token)
    token = re_alifMaqsurawithHamzah.sub('\u064A' , token)
    
    return token





# clean function will clean the arabic text from any other characters ( punctuations , numbers or one letter words. ( wow letter will be removed too :( ! )
def clean(words,punctuations=True,printable=False,hashtag=True):
    
    #Split joins letters Hell4 to [Hello ,4]
    words = re.split(r'([a-zA-Z]+)', words)
    words = ' '.join(words)
    words = re.split('(\d+)',words)
    words = ' '.join(words)
    
    # Remove maddah
    words = araby.strip_tatweel(words)
        
        # Here we use (split) to split on spaces (tokenize will split the hashtag words)
    words = words.split()
        
    wordsCopy = words.copy()
            
    for word in wordsCopy:
        # Remove one letter
        if len(word.strip(string.printable)) == 1:
            if word.isalpha():
                words.remove(word)
        # Remove Hashtags.
        elif hashtag and ( word.startswith('#') or word.endswith('#') ):
                words.remove(word)

    words = ' '.join(words)
            # Remove redundant letters (more than 2)
    words = re.sub(r'(.)\1{2,}', r'\1\1', words)
            
            # Remove Harakat
    words = araby.strip_tashkeel(words)


    if printable:
        # Remove string.printable
        # string.printable = 0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!
        #"#$%&'()*+,-./:;<=>?@[\]^_`{|}~
        words = words.translate(table_printable)

    if punctuations:
        # Remove  #"#$%&'()*+,-./:;<=>?@[\]^_`{|}~ 
        words = words.translate(table_punctuation)
        

            # Here we should use tokenize to split emoji from words
    wordsafterclean = araby.tokenize(words)
    return ' '.join(wordsafterclean)







# remove emojis from text.
def remove_emoji(em):
    
    em_split_emoji = emoji.get_emoji_regexp().split(em)
    em_split_whitespace = [substr.split() for substr in em_split_emoji]
    em_split = functools.reduce(operator.concat, em_split_whitespace)
    for i in em_split.copy():
        if i in emoji.UNICODE_EMOJI:
            em_split.remove(i)

    return ' '.join(em_split)


# split emoji from text.
def split_emoji(em):
    
    em_split_emoji = emoji.get_emoji_regexp().split(em)
    em_split_whitespace = [substr.split() for substr in em_split_emoji]
    em_split = functools.reduce(operator.concat, em_split_whitespace)
    em_split = ' '.join(em_split)
    
    return em_split






# numb will split numbers from words and switch Arabic numbers to english.
def numb(word):
    
    word = re.split('(\d+)',word)
    word = ' '.join(word)
    words = word.split()
    for i, w in enumerate(words):
        if w == 'Ù¡':
            words[i] = '1'
        elif w == 'Ù¢':
            words[i] == '2'
        elif w == 'Ù£':
            words[i] == '3'
        elif w == 'Ù¤':
            words[i] == '4'
        elif w == 'Ù¥':
            words[i] == '5'
        elif w == 'Ù¦':
            words[i] == '6'
        elif w == 'Ù§':
            words[i] == '7'
        elif w == 'Ù¨':
            words[i] == '8'
        elif w == 'Ù©':
            words[i] == '9'

    word = ' '.join(words)
    
    return word




stopwords_list = ['Ø¥Ø°', 'Ø¥Ø°Ø§', 'Ø¥Ø°Ù…Ø§', 'Ø¥Ø°Ù†', 'Ø£Ù', 'Ø£Ù‚Ù„', 'Ø£ÙƒØ«Ø±', 'Ø£Ù„Ø§', 'Ø¥Ù„Ø§', 'Ø§Ù„ØªÙŠ', 'Ø§Ù„Ø°ÙŠ', 'Ø§Ù„Ø°ÙŠÙ†', 'Ø§Ù„Ù„Ø§ØªÙŠ', 'Ø§Ù„Ù„Ø§Ø¦ÙŠ', 'Ø§Ù„Ù„ØªØ§Ù†', 'Ø§Ù„Ù„ØªÙŠØ§', 'Ø§Ù„Ù„ØªÙŠÙ†', 'Ø§Ù„Ù„Ø°Ø§Ù†', 'Ø§Ù„Ù„Ø°ÙŠÙ†', 'Ø§Ù„Ù„ÙˆØ§ØªÙŠ', 'Ø¥Ù„Ù‰', 'Ø¥Ù„ÙŠÙƒ', 'Ø¥Ù„ÙŠÙƒÙ…', 'Ø¥Ù„ÙŠÙƒÙ…Ø§', 'Ø¥Ù„ÙŠÙƒÙ†', 'Ø£Ù…', 'Ø£Ù…Ø§', 'Ø£Ù…Ø§', 'Ø¥Ù…Ø§', 'Ø£Ù†', 'Ø¥Ù†', 'Ø¥Ù†Ø§', 'Ø£Ù†Ø§', 'Ø£Ù†Øª', 'Ø£Ù†ØªÙ…', 'Ø£Ù†ØªÙ…Ø§', 'Ø£Ù†ØªÙ†', 'Ø¥Ù†Ù…Ø§', 'Ø¥Ù†Ù‡', 'Ø£Ù†Ù‰', 'Ø£Ù†Ù‰', 'Ø¢Ù‡', 'Ø¢Ù‡Ø§', 'Ø£Ùˆ', 'Ø£ÙˆÙ„Ø§Ø¡', 'Ø£ÙˆÙ„Ø¦Ùƒ', 'Ø£ÙˆÙ‡', 'Ø¢ÙŠ', 'Ø£ÙŠ', 'Ø£ÙŠÙ‡Ø§', 'Ø¥ÙŠ', 'Ø£ÙŠÙ†', 'Ø£ÙŠÙ†', 'Ø£ÙŠÙ†Ù…Ø§', 'Ø¥ÙŠÙ‡', 'Ø¨Ø®', 'Ø¨Ø³', 'Ø¨Ø¹Ø¯', 'Ø¨Ø¹Ø¶', 'Ø¨Ùƒ', 'Ø¨ÙƒÙ…', 'Ø¨ÙƒÙ…', 'Ø¨ÙƒÙ…Ø§', 'Ø¨ÙƒÙ†', 'Ø¨Ù„', 'Ø¨Ù„Ù‰', 'Ø¨Ù…Ø§', 'Ø¨Ù…Ø§Ø°Ø§', 'Ø¨Ù…Ù†', 'Ø¨Ù†Ø§', 'Ø¨Ù‡', 'Ø¨Ù‡Ø§', 'Ø¨Ù‡Ù…', 'Ø¨Ù‡Ù…Ø§', 'Ø¨Ù‡Ù†', 'Ø¨ÙŠ', 'Ø¨ÙŠÙ†', 'Ø¨ÙŠØ¯', 'ØªÙ„Ùƒ', 'ØªÙ„ÙƒÙ…', 'ØªÙ„ÙƒÙ…Ø§', 'ØªÙ‡', 'ØªÙŠ', 'ØªÙŠÙ†', 'ØªÙŠÙ†Ùƒ', 'Ø«Ù…', 'Ø«Ù…Ø©', 'Ø­Ø§Ø´Ø§', 'Ø­Ø¨Ø°Ø§', 'Ø­ØªÙ‰', 'Ø­ÙŠØ«', 'Ø­ÙŠØ«Ù…Ø§', 'Ø­ÙŠÙ†', 'Ø®Ù„Ø§', 'Ø¯ÙˆÙ†', 'Ø°Ø§', 'Ø°Ø§Øª', 'Ø°Ø§Ùƒ', 'Ø°Ø§Ù†', 'Ø°Ø§Ù†Ùƒ', 'Ø°Ù„Ùƒ', 'Ø°Ù„ÙƒÙ…', 'Ø°Ù„ÙƒÙ…Ø§', 'Ø°Ù„ÙƒÙ†', 'Ø°Ù‡', 'Ø°Ùˆ', 'Ø°ÙˆØ§', 'Ø°ÙˆØ§ØªØ§', 'Ø°ÙˆØ§ØªÙŠ', 'Ø°ÙŠ', 'Ø°ÙŠÙ†', 'Ø°ÙŠÙ†Ùƒ', 'Ø±ÙŠØ«', 'Ø³ÙˆÙ', 'Ø³ÙˆÙ‰', 'Ø´ØªØ§Ù†', 'Ø¹Ø¯Ø§', 'Ø¹Ø³Ù‰', 'Ø¹Ù„', 'Ø¹Ù„Ù‰', 'Ø¹Ù„ÙŠÙƒ', 'Ø¹Ù„ÙŠÙ‡', 'Ø¹Ù…Ø§', 'Ø¹Ù†', 'Ø¹Ù†Ø¯', 'ØºÙŠØ±', 'ÙØ¥Ø°Ø§', 'ÙØ¥Ù†', 'ÙÙ„Ø§', 'ÙÙ…Ù†', 'ÙÙŠ', 'ÙÙŠÙ…', 'ÙÙŠÙ…Ø§', 'ÙÙŠÙ‡', 'ÙÙŠÙ‡Ø§', 'Ù‚Ø¯', 'ÙƒØ£Ù†', 'ÙƒØ£Ù†Ù…Ø§', 'ÙƒØ£ÙŠ', 'ÙƒØ£ÙŠÙ†', 'ÙƒØ°Ø§', 'ÙƒØ°Ù„Ùƒ', 'ÙƒÙ„', 'ÙƒÙ„Ø§', 'ÙƒÙ„Ø§Ù‡Ù…Ø§', 'ÙƒÙ„ØªØ§', 'ÙƒÙ„Ù…Ø§', 'ÙƒÙ„ÙŠÙƒÙ…Ø§', 'ÙƒÙ„ÙŠÙ‡Ù…Ø§', 'ÙƒÙ…', 'ÙƒÙ…', 'ÙƒÙ…Ø§', 'ÙƒÙŠ', 'ÙƒÙŠØª', 'ÙƒÙŠÙ', 'ÙƒÙŠÙÙ…Ø§', 'Ù„Ø§', 'Ù„Ø§Ø³ÙŠÙ…Ø§', 'Ù„Ø¯Ù‰', 'Ù„Ø³Øª', 'Ù„Ø³ØªÙ…', 'Ù„Ø³ØªÙ…Ø§', 'Ù„Ø³ØªÙ†', 'Ù„Ø³Ù†', 'Ù„Ø³Ù†Ø§', 'Ù„Ø¹Ù„', 'Ù„Ùƒ', 'Ù„ÙƒÙ…', 'Ù„ÙƒÙ…Ø§', 'Ù„ÙƒÙ†', 'Ù„ÙƒÙ†Ù…Ø§', 'Ù„ÙƒÙŠ', 'Ù„ÙƒÙŠÙ„Ø§', 'Ù„Ù…', 'Ù„Ù…Ø§', 'Ù„Ù†', 'Ù„Ù†Ø§', 'Ù„Ù‡', 'Ù„Ù‡Ø§', 'Ù„Ù‡Ù…', 'Ù„Ù‡Ù…Ø§', 'Ù„Ù‡Ù†', 'Ù„Ùˆ', 'Ù„ÙˆÙ„Ø§', 'Ù„ÙˆÙ…Ø§', 'Ù„ÙŠ', 'Ù„Ø¦Ù†', 'Ù„ÙŠØª', 'Ù„ÙŠØ³', 'Ù„ÙŠØ³Ø§', 'Ù„ÙŠØ³Øª', 'Ù„ÙŠØ³ØªØ§', 'Ù„ÙŠØ³ÙˆØ§', 'Ù…Ø§', 'Ù…Ø§Ø°Ø§', 'Ù…ØªÙ‰', 'Ù…Ø°', 'Ù…Ø¹', 'Ù…Ù…Ø§', 'Ù…Ù…Ù†', 'Ù…Ù†', 'Ù…Ù†Ù‡', 'Ù…Ù†Ù‡Ø§', 'Ù…Ù†Ø°', 'Ù…Ù‡', 'Ù…Ù‡Ù…Ø§', 'Ù†Ø­Ù†', 'Ù†Ø­Ùˆ', 'Ù†Ø¹Ù…', 'Ù‡Ø§', 'Ù‡Ø§ØªØ§Ù†', 'Ù‡Ø§ØªÙ‡', 'Ù‡Ø§ØªÙŠ', 'Ù‡Ø§ØªÙŠÙ†', 'Ù‡Ø§Ùƒ', 'Ù‡Ø§Ù‡Ù†Ø§', 'Ù‡Ø°Ø§', 'Ù‡Ø°Ø§Ù†', 'Ù‡Ø°Ù‡', 'Ù‡Ø°ÙŠ', 'Ù‡Ø°ÙŠÙ†', 'Ù‡ÙƒØ°Ø§', 'Ù‡Ù„', 'Ù‡Ù„Ø§', 'Ù‡Ù…', 'Ù‡Ù…Ø§', 'Ù‡Ù†', 'Ù‡Ù†Ø§', 'Ù‡Ù†Ø§Ùƒ', 'Ù‡Ù†Ø§Ù„Ùƒ', 'Ù‡Ùˆ', 'Ù‡Ø¤Ù„Ø§Ø¡', 'Ù‡ÙŠ', 'Ù‡ÙŠØ§', 'Ù‡ÙŠØª', 'Ù‡ÙŠÙ‡Ø§Øª', 'ÙˆØ§Ù„Ø°ÙŠ', 'ÙˆØ§Ù„Ø°ÙŠÙ†', 'ÙˆØ¥Ø°', 'ÙˆØ¥Ø°Ø§', 'ÙˆØ¥Ù†', 'ÙˆÙ„Ø§', 'ÙˆÙ„ÙƒÙ†', 'ÙˆÙ„Ùˆ', 'ÙˆÙ…Ø§', 'ÙˆÙ…Ù†', 'ÙˆÙ‡Ùˆ', 'ÙŠØ§']


stopwords_list = ' '.join(stopwords_list)
stopwords_list = clean(stopwords_list)

stopwords_list = stopwords_list.split()

for i in stopwords_list:
    stopwords_list.append(norm(i))
    
    
def rstop_words(sentence):
    words = [i for i in sentence.lower().split() if i not in stopwords_list]

    return ' '.join(words)
